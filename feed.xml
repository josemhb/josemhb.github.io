<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>Josemhb Blog</title>
    <link href="https://josemhb.github.io/feed.xml" rel="self" />
    <link href="https://josemhb.github.io" />
    <updated>2024-03-31T19:05:36+02:00</updated>
    <author>
        <name>Jose Manuel Hernandez</name>
    </author>
    <id>https://josemhb.github.io</id>

    <entry>
        <title>Solucionar conflictos con vibs actualizando de VMware ESXi 5.5 a 6.5</title>
        <author>
            <name>Jose Manuel Hernandez</name>
        </author>
        <link href="https://josemhb.github.io/solucionar-conflictos-con-vibs-actualizando-de-vmware-esxi-55-a-65.html"/>
        <id>https://josemhb.github.io/solucionar-conflictos-con-vibs-actualizando-de-vmware-esxi-55-a-65.html</id>
        <media:content url="https://josemhb.github.io/media/posts/5/server3.jpg" medium="image" />
            <category term="VMware"/>

        <updated>2018-08-03T10:42:00+02:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://josemhb.github.io/media/posts/5/server3.jpg" alt="" />
                    En la actualización de ESXi 5.5 a 6.5 es probable que te hayas topado con algún error al tener una vib incompatible y el proceso te falle con un error&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://josemhb.github.io/media/posts/5/server3.jpg" class="type:primaryImage" alt="" /></p>
                <p data-original-attrs="{&quot;style&quot;:&quot;&quot;}">En la actualización de ESXi 5.5 a 6.5 es probable que te hayas topado con algún error al tener una vib incompatible y el proceso te falle con un error del tipo "<strong>Conflicting_vibs Error</strong>".</p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Solucionar este error es sencillo:</p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}">1º Anota el nombre de las vibs que te generan problemas, ten en cuenta que el nombre que muestra el error es el nombre largo de la vib.<br><br><a data-original-attrs="{&quot;data-original-href&quot;:&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCM_EjXdUkDZC-nQA9ac1oJTTr1Frh03BshIVk2r-0kCQDX3e0gM_FAYJxgTQd1H3X1DEVRLgeLlLc45whAvx5q0RAz08pELs12-6rmK3w6s0OVrSkS6_efBMo0-bwKFgcTfS5MFQvwaTM/s1600/error.png&quot;,&quot;style&quot;:&quot;&quot;}" href="https://www.blogger.com/u/1/#"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCM_EjXdUkDZC-nQA9ac1oJTTr1Frh03BshIVk2r-0kCQDX3e0gM_FAYJxgTQd1H3X1DEVRLgeLlLc45whAvx5q0RAz08pELs12-6rmK3w6s0OVrSkS6_efBMo0-bwKFgcTfS5MFQvwaTM/s400/error.png" width="400" height="231" border="0" data-original-height="319" data-original-width="551" data-is-external-image="true"></a></p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}">2º Entra por SSH a tu host ESXi y lista las vibs instaladas:<br><br><code>  esxcli software vib list|grep Mel</code></p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}">3º Anota el nombre corto de la vib que necesitarás en el siguiente paso para eliminarla.</p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}">4º Elimina la vib que da conflicto con la nueva versión de ESXi:</p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> <code> esxcli software vib remove --vibname=[nombre_corto_vib]</code></p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}">ejemplo:</p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}"><code>  esxcli software vib remove --vibname=scsi-lpfc820</code></p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Repite este paso con todas las vibs conflictivas.</p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}"><br>5º Reinicia el host ESXi. <strong>Es necesario que hagas un reboot desde ESXi no funciona un apagado forzoso o un reset hardware</strong>.</p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}">6º Ya por último, prueba actualizar tu ESXi.</p>
<p data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>¿Que es LBaaS?</title>
        <author>
            <name>Jose Manuel Hernandez</name>
        </author>
        <link href="https://josemhb.github.io/que-es-lbaas.html"/>
        <id>https://josemhb.github.io/que-es-lbaas.html</id>
        <media:content url="https://josemhb.github.io/media/posts/6/network.jpg" medium="image" />
            <category term="Openstack"/>

        <updated>2016-09-26T12:55:00+02:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://josemhb.github.io/media/posts/6/network.jpg" alt="networking" />
                    LBaaS es el servicio de Openstack que nos permite disponer de un balanceador de carga como servicio. Para quienes no sepan que es un balanceador de carga, es una tecnología&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://josemhb.github.io/media/posts/6/network.jpg" class="type:primaryImage" alt="networking" /></p>
                <h2 data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </h2>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"><strong>LBaaS</strong> es el servicio de Openstack que nos permite disponer de un balanceador de carga como servicio.</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Para quienes no sepan que es un balanceador de carga, es una tecnología que permite disponer de un servidor virtual con una IP, este servidor virtual está respaldado por un grupo de servidores reales que proporcionan algún recurso a los clientes. La función del balanceador es distribuir las peticiones de cliente a través del grupo de servidores, esta distribución se puede hacer en base a varios algoritmos tal como:</div>
<ul data-original-attrs="{&quot;style&quot;:&quot;&quot;}">
<li><strong>Round Robin:</strong> Selecciona servidores secuencialmente. Es el algoritmo predeterminado.</li>
<li><strong>Leastconn:</strong> Selecciona el servidor con el menor número de conexiones – se recomienda para las sesiones más largas.</li>
<li><strong>Source:</strong> Selecciona el servidor que se desea utilizar en función de un hash de la IP de origen, es decir, la dirección IP del usuario. Es un método para asegurar que un usuario se conecte siempre al mismo servidor.</li>
<li><strong>Sticky Sessions:</strong> Algunas aplicaciones requieren que un usuario continúe accediendo al mismo servidor de backend. Esta persistencia se logra a través de las sticky sessions, utilizando el parámetro appsession en el backend que lo requiera.</li>
</ul>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Otra características de los balanceadores es la detección de fallos en un servidor y su capacidad de eliminar una ruta hacia este, cuando esto sucede los clientes no se enteran del fallo en el servidor real ya que se redirigen a otro servidor operativo.</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div class="separator" data-original-attrs="{&quot;style&quot;:&quot;&quot;}"><a data-original-attrs="{&quot;data-original-href&quot;:&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYERMpJ_arRbteP4DOZEoZrZ6vwy5pRiqe98nPvpS_lUDrNneLXbbfJus0fW0u-d6C355XtVZ7JL_VayrLb8P_A8gFKjYH3wQVc2cI8zlL6YZe3_3JCijXCBa9_QKO3l8jhFsTxMF6nIF9/s1600/loadbalancer.png&quot;,&quot;style&quot;:&quot;&quot;}" href="https://www.blogger.com/u/1/#"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYERMpJ_arRbteP4DOZEoZrZ6vwy5pRiqe98nPvpS_lUDrNneLXbbfJus0fW0u-d6C355XtVZ7JL_VayrLb8P_A8gFKjYH3wQVc2cI8zlL6YZe3_3JCijXCBa9_QKO3l8jhFsTxMF6nIF9/s400/loadbalancer.png" width="400" height="161" border="0" data-original-height="254" data-original-width="630" data-is-external-image="true"></a></div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">En resumen, las funciones principales de los balanceadores de carga son atender las solicitudes de los clientes mitigando los problemas de cuellos de botella al repartir la carga entre varios nodos y evitar los fallos cuando un servidor de un pool cae, aportando alta disponibilidad.</div>
<h2 data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </h2>
<h2 data-original-attrs="{&quot;style&quot;:&quot;&quot;}">OpenStack LBaaS</h2>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Actualmente en Openstack la implementación de balanceadores de carga corre a cargo del proyecto Neutron el cuál implementa la versión 2 de <strong>LBaaS</strong>. Este servicio permite configurar un balanceador que se ejecuta fuera de las instancias y dirige el tráfico hacia ellas en función de la configuración realizada.</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">El servicio <strong>OpenStack LBaaS</strong> lo presta un conocido software de código abierto como es <strong>HAProxy</strong>. HAProxy proporciona balanceo de carga TCP/HTTP y es el que utiliza por defecto OpenStack. No obstante, desde la v2 existe la opción de utilizar una implementación alternativa de LBaaS llamada [<a data-original-attrs="{&quot;data-original-href&quot;:&quot;https://wiki.openstack.org/wiki/Octavia&quot;}" href="https://www.blogger.com/u/1/#">Octavia</a>] la cuál está totalmente integrada con Neutron y LBaaS v2.</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">A continuación podemos ver un vídeo demostrativo de LBaaS por gentileza de Rackspace:</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube.com/embed/C512TQIplfs" allowfullscreen="allowfullscreen"></iframe></figure>
</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Espero que os haya resultado interesante.</div>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Integración Openstack – Nutanix</title>
        <author>
            <name>Jose Manuel Hernandez</name>
        </author>
        <link href="https://josemhb.github.io/integracion-openstack-nutanix.html"/>
        <id>https://josemhb.github.io/integracion-openstack-nutanix.html</id>
        <media:content url="https://josemhb.github.io/media/posts/7/server4.jpg" medium="image" />
            <category term="Openstack"/>
            <category term="Nutanix"/>

        <updated>2016-07-07T12:04:00+02:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://josemhb.github.io/media/posts/7/server4.jpg" alt="" />
                    Últimamente he estado algo atareado y con poco tiempo para escribir, esta vez quería hablar sobre la sencilla integración de los equipos hiperconvergentes de Nutanix con Openstack. Hace poco tuvimos&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://josemhb.github.io/media/posts/7/server4.jpg" class="type:primaryImage" alt="" /></p>
                <div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"><a data-original-attrs="{&quot;data-original-href&quot;:&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxCGe45Y2wGYCgV3DP6g8w-OpRQI_VeR0KOlZNA37fUpbqqW9-b7mcgaOhgqUWyxC_rnw5Z3OGmJVBXZ15rivcv0CwtQUIiI_aQJ8BQjTnV4EUXX8RlElM7xFMdCAzEumm6n3g9kuaYcKA/s1600/Opens-nutanix-768x216.png&quot;,&quot;style&quot;:&quot;&quot;}" href="https://www.blogger.com/u/1/#"><figure class="align-center"><img loading="lazy"  src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxCGe45Y2wGYCgV3DP6g8w-OpRQI_VeR0KOlZNA37fUpbqqW9-b7mcgaOhgqUWyxC_rnw5Z3OGmJVBXZ15rivcv0CwtQUIiI_aQJ8BQjTnV4EUXX8RlElM7xFMdCAzEumm6n3g9kuaYcKA/s400/Opens-nutanix-768x216.png" width="400" height="110" border="0" data-original-height="216" data-original-width="768" data-is-external-image="true"></figure></a></div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Últimamente he estado algo atareado y con poco tiempo para escribir, esta vez quería hablar sobre la sencilla integración de los equipos hiperconvergentes de <strong>Nutanix</strong> con <strong>Openstack</strong>.</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Hace poco tuvimos el gusto de probar durante un mes un par de equipos por gentileza de Nutanix Iberia y la verdad que funcionan de maravilla.</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Si aún no sabes de que va el tema de la hiperconvergencia y Nutanix, te recomiendo que leas los siguientes artículos:</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"><a data-original-attrs="{&quot;data-original-href&quot;:&quot;http://www.tundra-it.com/que-es-nutanix/&quot;}" href="https://www.blogger.com/u/1/#">¿Que es Nutanix?</a></div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"><a data-original-attrs="{&quot;data-original-href&quot;:&quot;http://blog.e2h.net/2013/04/30/nutanix-la-evolucion-del-centro-de-datos/&quot;}" href="https://www.blogger.com/u/1/#">Nutanix la evolución del centro de datos</a></div>
<h2 data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Arquitectura</h2>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div class="separator" data-original-attrs="{&quot;style&quot;:&quot;&quot;}"><a data-original-attrs="{&quot;data-original-href&quot;:&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuahLqSps1_EmalZ0sbb2oQTKf2Sks9cND_B4oUzgDXT0Kyz4G45B5XwFgxCaWMFk9lxAyCxjVs9gXOiHIXZXMr2lid1tLZ_0SJcckWdwD1dVQC0VKzTt7d-bN9NjW6-M-n4Xw1KeUamCs/s1600/nutanix-2-768x311.png&quot;,&quot;style&quot;:&quot;&quot;}" href="https://www.blogger.com/u/1/#"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuahLqSps1_EmalZ0sbb2oQTKf2Sks9cND_B4oUzgDXT0Kyz4G45B5XwFgxCaWMFk9lxAyCxjVs9gXOiHIXZXMr2lid1tLZ_0SJcckWdwD1dVQC0VKzTt7d-bN9NjW6-M-n4Xw1KeUamCs/s640/nutanix-2-768x311.png" width="640" height="258" border="0" data-original-height="311" data-original-width="768" data-is-external-image="true"></a></div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Nutanix ha trabajado bastante la integración con Openstack y para conseguirla ha desarrollado dos componentes que paso a describir:</div>
<ul data-original-attrs="{&quot;style&quot;:&quot;&quot;}">
<li><strong>Acropolis OpenStack Driver:</strong><br>Son drivers que puedes configurar en los nodos de control de Openstack o usar los que trae la máquina OVM preinstalados, su cometido es traducir las llamadas del nodo de control Openstack a la API REST nativa de Acrópolis.</li>
</ul>
<div class="separator" data-original-attrs="{&quot;style&quot;:&quot;&quot;}"><a data-original-attrs="{&quot;data-original-href&quot;:&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFuMcxQYWRSNARC4ydY3awCuy3mZXrIu1pRffa85LeBkiKjBsLMZPPTVqITR8FgSvoRz1EzIYtXjkJCrqZTWPddg61Fj_mCyybmhzGFd2tL2yEGlg1duXE_whbeiM-BdD3VizTgpqQoyp5/s1600/nutanix51-768x194.png&quot;,&quot;style&quot;:&quot;&quot;}" href="https://www.blogger.com/u/1/#"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFuMcxQYWRSNARC4ydY3awCuy3mZXrIu1pRffa85LeBkiKjBsLMZPPTVqITR8FgSvoRz1EzIYtXjkJCrqZTWPddg61Fj_mCyybmhzGFd2tL2yEGlg1duXE_whbeiM-BdD3VizTgpqQoyp5/s640/nutanix51-768x194.png" width="640" height="160" border="0" data-original-height="194" data-original-width="768" data-is-external-image="true"></a></div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<ul data-original-attrs="{&quot;style&quot;:&quot;&quot;}">
<li><strong>Acropolis OpenStack Services VM (OVM)</strong>:<br>Es una máquina virtual proporcionada por Nutanix que se ejecuta en cualquier nodo dentro de nuestro clúster y viene configurada con el driver de Acropolis para comunicar con Openstack, OVM facilita el despliegue de Openstack ya que trae los servicios de un controlador Openstack pre-instalados y listos para funcionar. Una vez configurada el cliente puede conectarse de modo habitual al controlador vía Horizon, SDK, CLI o API.</li>
</ul>
<div class="separator" data-original-attrs="{&quot;style&quot;:&quot;&quot;}"><a data-original-attrs="{&quot;data-original-href&quot;:&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjecYbuTrvGOYGihQXNXUTrUc4vSIsH6dr4dguFImCq5j7oveAr6za7garBkfPNrWDK-LCLeBjtL993i_9qxE9yw4bsBDpbcm8EkVBX74j_5asS37RPAFg97ZZpgguE1_7ccdK6PJ3xZ-S5/s1600/nutanix04-768x323.png&quot;,&quot;style&quot;:&quot;&quot;}" href="https://www.blogger.com/u/1/#"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjecYbuTrvGOYGihQXNXUTrUc4vSIsH6dr4dguFImCq5j7oveAr6za7garBkfPNrWDK-LCLeBjtL993i_9qxE9yw4bsBDpbcm8EkVBX74j_5asS37RPAFg97ZZpgguE1_7ccdK6PJ3xZ-S5/s640/nutanix04-768x323.png" width="640" height="267" border="0" data-original-height="323" data-original-width="768" data-is-external-image="true"></a></div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<h3 data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Ventajas de Openstack con Nutanix</h3>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">A continuación describo algunas de las <strong>ventajas</strong> de desplegar Openstack con Nutanix:</div>
<ul data-original-attrs="{&quot;style&quot;:&quot;&quot;}">
<li><strong>Instalación rápida y sencilla: </strong>Al usar bloques HCI reducimos los tiempos de puesta en marcha y simplificamos la instalación de cómputo y almacenamiento.</li>
<li><strong>Scale-out sencillo:</strong> Se puede escalar fácilmente tanto en computación como en almacenamiento expandiendo el clúster con un solo clic.</li>
<li><strong>Autotiering con discos SSD y SATA:</strong> Capacidad automática de almacenar los datos calientes en disco SSD y los datos fríos en disco SATA.</li>
<li><strong>Compresión y Deduplicación en almacenamiento:</strong> Permitiendo un ahorro de espacio considerable.</li>
<li><strong>Integración Nova + Glance con NDFS Fast-clones:</strong> Al aprovechar esta capacidad se reducen drásticamente los tiempos de creación de instancias, permitiendo crear decenas de instancias totalmente operativas en menos de un minuto.</li>
<li><strong>Alta disponibilidad y balanceo de carga:</strong> Ideal para aplicaciones Legacy las cuales no son cloud native y no pueden funcionar en modo stateless.</li>
<li><strong>Monitorización end to end de toda la infraestructura:</strong> Permitiendo detectar fallos o problemas en cualquier componente hardware.</li>
<li><strong>Gestión de la capacidad:</strong> Prism ahora también ofrece la planificación de capacidad predicativa, lo que le permite ver fácilmente en base a sus tasas de crecimiento cuándo necesitará ampliar la capacidad de su infraestructura.</li>
</ul>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Aquí os dejo un pequeño vídeo demostrativo que hice para ver lo bien y rápido que funciona Openstack sobre plataforma hiperconvergente Nutanix.<br><br>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube.com/embed/vudzPvU-UvY" allowfullscreen="allowfullscreen"></iframe></figure>
</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">La verdad que será una opción a tener en cuenta en futuros despliegues de Openstack.</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Mas info en <a data-original-attrs="{&quot;data-original-href&quot;:&quot;http://nutanix.com/&quot;}" href="https://www.blogger.com/u/1/#">Nutanix.com</a></div>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Reutilizar una IP flotante en Openstack</title>
        <author>
            <name>Jose Manuel Hernandez</name>
        </author>
        <link href="https://josemhb.github.io/reutilizar-una-ip-flotante-en-openstack.html"/>
        <id>https://josemhb.github.io/reutilizar-una-ip-flotante-en-openstack.html</id>
        <media:content url="https://josemhb.github.io/media/posts/4/server2.jpg" medium="image" />
            <category term="Openstack"/>

        <updated>2016-01-15T22:37:00+01:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://josemhb.github.io/media/posts/4/server2.jpg" alt="" />
                    Hoy quería compartir un problema que he tenido en una instalación de Openstack. Como sabéis, Openstack Neutron asigna IPs flotantes de manera consecutiva y hasta que no se consuman todas&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://josemhb.github.io/media/posts/4/server2.jpg" class="type:primaryImage" alt="" /></p>
                <div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Hoy quería compartir un problema que he tenido en una instalación de Openstack. Como sabéis, Openstack Neutron asigna IPs flotantes de manera consecutiva y hasta que no se consuman todas las IPs del pool no empieza a reciclar las IPs en desuso. En mi caso para evitar modificar la configuración de firewalls, DNS y demás necesitaba rehusar esta misma dirección IP para una instancia.</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"><br>Tras desasignar la IP y eliminarse esta del Tenant, necesitaba asignarla de nuevo, a continuación os detallo como lo he realizado.</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Esta configuración se hace a través de la base de datos de nuestra infraestructura así que vamos a entrar desde un nodo de control a MySQL:</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">
<pre class="screen">root@node-11:~# mysql -u root
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 525603
Server version: 5.6.16-2~u14.04+mos2-log (Ubuntu), wsrep_25.5.rXXXX

Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.</pre>
</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Usamos la BD de Neutron:</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">
<pre class="screen">mysql&gt; use neutron;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed</pre>
</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Identificamos los pools de IPs que tenemos:</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">
<pre class="screen">mysql&gt; select * from ipavailabilityranges;
+--------------------------------------+-------------+--------------+
| allocation_pool_id                   | first_ip    | last_ip      |
+--------------------------------------+-------------+--------------+
| 1fbbfa85-bc2e-447a-8ddb-8436bb783706 | 10.50.1.48  | 10.50.1.254  |
| 3271e259-2767-4234-816b-101ecf56e152 | 10.50.2.5   | 10.50.2.254  |
| 5ebbce7f-21d6-43a6-be65-5165347eb29d | 10.20.15.35 | 10.20.15.254 |
| d3f1f050-b3c7-4ac5-9318-33c194018e45 | 10.20.10.59 | 10.20.11.254 |
| db102e26-4697-4957-8e68-0c24f656d1fd | 10.50.4.9   | 10.50.4.254  |
+--------------------------------------+-------------+--------------+
5 rows in set (0.00 sec)</pre>
</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Establecemos dentro del pool en cuestión la IP que queremos que sea asignada:</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">
<pre class="screen">mysql&gt; UPDATE ipavailabilityranges SET first_ip = '10.20.10.24' 
WHERE allocation_pool_id='d3f1f050-b3c7-4ac5-9318-33c194018e45';
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0</pre>
</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">&gt;NOTA: Es importante tener control sobre la asignación de IPs en la infraestructura y mientras realicemos este proceso no se soliciten IPs flotantes.</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Asociamos la IP flotante a nuestra instancia:</div>
<div class="separator" data-original-attrs="{&quot;style&quot;:&quot;&quot;}"><a data-original-attrs="{&quot;data-original-href&quot;:&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYkrznMrOy0ql5_OCDfLlSS7eYEYdhKB6oX8QseCBjIkfX3GQscXUISSI8_tZjOnrzi0oUepzrwYA-LAPDOhg1GYhRSepAc9apGd71bP6IFmI6MaAzQA-VyrAiETsY6sf8aWwVcDZKIOuF/s1600/ip_floating-1024x311.png&quot;,&quot;style&quot;:&quot;&quot;}" href="https://www.blogger.com/u/1/#"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYkrznMrOy0ql5_OCDfLlSS7eYEYdhKB6oX8QseCBjIkfX3GQscXUISSI8_tZjOnrzi0oUepzrwYA-LAPDOhg1GYhRSepAc9apGd71bP6IFmI6MaAzQA-VyrAiETsY6sf8aWwVcDZKIOuF/s640/ip_floating-1024x311.png" width="640" height="194" border="0" data-original-height="311" data-original-width="1024" data-is-external-image="true"></a></div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Devolvemos la primera IP disponible a la que estaba establecida inicialmente para no tener problemas:</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">
<pre class="screen">mysql&gt; UPDATE ipavailabilityranges SET first_ip = '10.20.10.59' 
WHERE allocation_pool_id='d3f1f050-b3c7-4ac5-9318-33c194018e45';
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0</pre>
</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Con este workaround podremos disponer de una IP reciclada de nuevo en nuestra instancia.</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
            ]]>
        </content>
    </entry>
    <entry>
        <title>[TIP] Habilitar Transparent Page Sharing (TPS) en VMware ESXi </title>
        <author>
            <name>Jose Manuel Hernandez</name>
        </author>
        <link href="https://josemhb.github.io/tip-habilitar-transparent-page-sharing-tps-en-vmware-esxi.html"/>
        <id>https://josemhb.github.io/tip-habilitar-transparent-page-sharing-tps-en-vmware-esxi.html</id>
        <media:content url="https://josemhb.github.io/media/posts/3/server1.jpg" medium="image" />
            <category term="VMware"/>

        <updated>2015-11-11T18:29:00+01:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://josemhb.github.io/media/posts/3/server1.jpg" alt="server" />
                    ¡Hola a todos! tras unos meses con mucho trabajo y pocas horas de descanso, por el nacimiento de mi segundo hijo, he vuelto a sacar tiempo para escribir varios artículos.
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://josemhb.github.io/media/posts/3/server1.jpg" class="type:primaryImage" alt="server" /></p>
                <div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">¡Hola a todos! tras unos meses con mucho trabajo y pocas horas de descanso, por el nacimiento de mi segundo hijo, he vuelto a sacar tiempo para escribir varios artículos. Este es un pequeño TIP para recordaros algo sobre <strong>TPS</strong> de <strong>ESXi</strong>.</div>
<p><br><br></p>
<div class="separator" data-original-attrs="{&quot;style&quot;:&quot;&quot;}"><a data-original-attrs="{&quot;data-original-href&quot;:&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQPEmPKuy0dAwFabdjIl1ErvEKEs5haRFMK22rJM1m3NelWpt-pbjFj1G1kRy-niZPKxaXXDWvJ9FYyGBK1NM6Qr2MERA_Of9-BLh78bQhiOo-oWEsmLBDFBcmAcd-K3hqFuu-F14cO42k/s1600/tps.JPG&quot;,&quot;style&quot;:&quot;&quot;}" href="https://www.blogger.com/u/1/#"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQPEmPKuy0dAwFabdjIl1ErvEKEs5haRFMK22rJM1m3NelWpt-pbjFj1G1kRy-niZPKxaXXDWvJ9FYyGBK1NM6Qr2MERA_Of9-BLh78bQhiOo-oWEsmLBDFBcmAcd-K3hqFuu-F14cO42k/s1600/tps.JPG" border="0" data-original-height="258" data-original-width="316" data-is-external-image="true"></a></div>
<p> </p>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Como sabéis, <strong>TPS (Transparent Page Sharing)</strong> es un mecanismo que nos permite ahorrar en el uso de memoria de cada host ESXi guardando en memoria física solo bloques únicos, en muchos casos este ahorro es bastante significativo.</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">A partir de la [<a data-original-attrs="{&quot;data-original-href&quot;:&quot;http://kb.vmware.com/selfservice/search.do?cmd=displayKC&amp;docType=kc&amp;docTypeID=DT_KB_1_1&amp;externalId=2080735&quot;}" href="https://www.blogger.com/u/1/#">KB2080735</a>] por motivos de seguridad se decidió deshabilitar por defecto TPS en las posteriores versiones de ESXi. Estos cambios están relacionados con una investigación académica reciente que aprovecha transparente Sharing Página (TPS) para obtener acceso no autorizado a los datos en memoria en determinadas condiciones.</div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}"> </div>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">En algunas instalaciones normales que he estado revisando me he dado cuenta de que no estaba habilitado TPS. En muchos casos y en entornos controlados, si este pequeño problema de seguridad no te afecta, recuerda habilitar TPS en tu plataforma vSphere para optimizar el uso de memoria en tu ESXi.</div>
<p><br>Puedes hacerlo de dos modos:<br><br><strong>Por GUI</strong>:<br><br>1 – Entrar con el cliente vsphere y seleccionar el host ESXi<br>2 – Seleccionar la ficha configuración y luego configuración avanzada (advanced settings)<br>3 – Haga clic en **Mem**<br>4 – Buscar la variable **Mem.ShareForceSalting** y establezca el valor en **1** para habilitarlo.<br>5 – Pulsar aceptar<br>6 – Migrar todas las máquinas a otro host para poder reiniciar el ESXi sobre el que hemos realizado el cambio.<br>7 – Reiniciar<br><br><strong>Vía Powercli (si tienes muchos esxi):</strong><br><br></p>
<div data-original-attrs="{&quot;style&quot;:&quot;&quot;}">Si tienes muchos hosts puede que el método anterior sea poco óptimo y lento, para ello VMware ha creado un script (pshare-salting.ps1) que puedes lanzar desde PowerCLI y aplicar este cambio sobre todos los hosts:</div>
<p><br>*.\pshare-salting.ps1 [vcenter IP/hostname] -s*<br><br><strong>script:</strong></p>
<pre class="screen"># Configure pshare salting options
# - Enables / Disables the salting. 
#

$reboot = 0
$enable = 0
$disable = 0
if ($args.count -gt 0) {
   $vCenter = $args[ 0 ]
   for ( $i = 1; $i -lt $args.count; $i++ ) {
      if ($args[ $i ] -eq "-s") {
         $enable = 1
      }
      if ($args[ $i ] -eq "-o") {
         $disable = 1
      }
   }
   # override
   if ($enable -eq 1) {
      $disable = 0
   }
}

# we should specify either -s or -o
if ($enable -eq 0 ) {
  if ($disable -eq 0 ) {
     Write-Host "Usage: ./pshare-salting.ps1  &lt;-s/-o&gt;"
     Write-Host "-s: turn-on pshare salting (default)"
     Write-Host "-o: turn-off pshare salting\n"
     return
   }
}

Connect-VIServer $vCenter

$esxHosts = Get-VMHost | Sort Name
foreach($esx in $esxHosts){
      # Revert ShareScaGHz to default
      Set-VMHostAdvancedConfiguration -VMHost $esx -Name Mem.ShareScanGHz -Value 4 -Confirm:$false    
   if ($enable -eq 1) {
      Write-Host "Enabling PageShare Salting on $esx"
      $val = (Get-VMHostAdvancedConfiguration -VMHost $esx -Name Mem.ShareForceSalting).Values
      Set-VMHostAdvancedConfiguration -VMHost $esx -Name Mem.ShareForceSalting -Value 2 -Confirm:$false    
   } else {
      Write-Host "Disabling PageShare Salting on $esx"
      $val = (Get-VMHostAdvancedConfiguration -VMHost $esx -Name Mem.ShareForceSalting.).Values
      Set-VMHostAdvancedConfiguration -VMHost $esx -Name Mem.ShareForceSalting -Value 0 -Confirm:$false      
   }
}
</pre>
<p>Con esto tendrás todos tus hosts con TPS habilitado.<br><br>Hasta la próxima.<br><br></p>
            ]]>
        </content>
    </entry>
</feed>
